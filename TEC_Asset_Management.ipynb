{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b8e6ef",
   "metadata": {},
   "source": [
    "# TEC Asset Management System ğŸ›ï¸\n",
    "\n",
    "## The Elidoras Codex - Digital Asset Organization & Analysis\n",
    "\n",
    "**Mission:** Analyze and manage the foundational media assets for TEC's constitutional framework implementation.\n",
    "\n",
    "**Scope:** \n",
    "- Parse the ASSET_MANIFEST.md for structured data\n",
    "- Create digital asset database and validation system  \n",
    "- Generate reports for asset organization and integration\n",
    "- Prepare assets for AxiomEngine validation framework\n",
    "\n",
    "**Assets Overview:**\n",
    "- **Video Assets:** Core mission videos and narrative content\n",
    "- **Audio Assets:** Blueprint doctrine and philosophical commentary\n",
    "- **Status:** Implementation Phase - Organized & Ready\n",
    "\n",
    "---\n",
    "\n",
    "This notebook transforms chaotic creative energy into systematic asset management infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# For markdown parsing and file operations\n",
    "try:\n",
    "    import markdown\n",
    "    print(\"âœ… Markdown library available\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Installing markdown library...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'markdown'])\n",
    "    import markdown\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"ğŸ›ï¸ TEC Asset Management Libraries Loaded\")\n",
    "print(f\"ğŸ“… Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"âš¡ Ready to process foundational media assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e36153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Asset Manifest File\n",
    "def read_asset_manifest(file_path: str = \"assets/ASSET_MANIFEST.md\") -> str:\n",
    "    \"\"\"Read the TEC Asset Manifest markdown file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        print(f\"âœ… Successfully read manifest: {file_path}\")\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Manifest file not found: {file_path}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error reading manifest: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_manifest_structure(content: str) -> Dict:\n",
    "    \"\"\"Parse the markdown structure to extract organizational data\"\"\"\n",
    "    \n",
    "    # Extract directory structure\n",
    "    dir_structure_match = re.search(r'```\\nassets/\\n(.*?)\\n```', content, re.DOTALL)\n",
    "    directory_info = dir_structure_match.group(1) if dir_structure_match else \"\"\n",
    "    \n",
    "    # Extract sections\n",
    "    sections = {\n",
    "        'title': re.search(r'^#\\s+(.+)', content, re.MULTILINE),\n",
    "        'status': \"Implementation Ready\" if \"Implementation Ready\" in content else \"Unknown\",\n",
    "        'video_section': re.search(r'#### Video Assets.*?(?=####|\\Z)', content, re.DOTALL),\n",
    "        'audio_section': re.search(r'#### Audio Assets.*?(?=##|\\Z)', content, re.DOTALL),\n",
    "        'integration_notes': re.search(r'## Asset Integration Notes(.*?)(?=##|\\Z)', content, re.DOTALL)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'raw_content': content,\n",
    "        'directory_structure': directory_info,\n",
    "        'sections': sections,\n",
    "        'parsed_date': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Read and parse the manifest\n",
    "manifest_content = read_asset_manifest()\n",
    "if manifest_content:\n",
    "    parsed_manifest = parse_manifest_structure(manifest_content)\n",
    "    print(\"ğŸ“‹ Manifest Structure Parsed:\")\n",
    "    print(f\"   - Status: {parsed_manifest['sections']['status']}\")\n",
    "    print(f\"   - Video Section: {'âœ… Found' if parsed_manifest['sections']['video_section'] else 'âŒ Missing'}\")\n",
    "    print(f\"   - Audio Section: {'âœ… Found' if parsed_manifest['sections']['audio_section'] else 'âŒ Missing'}\")\n",
    "else:\n",
    "    print(\"âŒ Failed to parse manifest - creating empty structure\")\n",
    "    parsed_manifest = {'raw_content': '', 'sections': {}, 'directory_structure': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c41e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Asset Metadata\n",
    "def extract_asset_metadata(manifest_data: Dict) -> List[Dict]:\n",
    "    \"\"\"Extract detailed metadata for each asset from the manifest\"\"\"\n",
    "    \n",
    "    assets = []\n",
    "    content = manifest_data['raw_content']\n",
    "    \n",
    "    # Pattern to match asset entries\n",
    "    asset_pattern = r'\\*\\*(\\d+)\\.\\s+(.+?)\\*\\*\\n((?:- \\*\\*.*?\\n)*)'\n",
    "    \n",
    "    # Extract video assets\n",
    "    video_section = manifest_data['sections'].get('video_section')\n",
    "    if video_section:\n",
    "        video_matches = re.findall(asset_pattern, video_section.group(0))\n",
    "        for match in video_matches:\n",
    "            asset_id, filename, metadata_text = match\n",
    "            \n",
    "            # Parse metadata fields\n",
    "            metadata = {'category': 'video', 'id': asset_id, 'filename': filename.strip()}\n",
    "            \n",
    "            for line in metadata_text.split('\\n'):\n",
    "                if '**Type:**' in line:\n",
    "                    metadata['type'] = re.search(r'\\*\\*Type:\\*\\*\\s*(.+)', line).group(1).strip()\n",
    "                elif '**Purpose:**' in line:\n",
    "                    metadata['purpose'] = re.search(r'\\*\\*Purpose:\\*\\*\\s*(.+)', line).group(1).strip()\n",
    "                elif '**Content:**' in line:\n",
    "                    metadata['content_description'] = re.search(r'\\*\\*Content:\\*\\*\\s*(.+)', line).group(1).strip()\n",
    "                elif '**Status:**' in line:\n",
    "                    metadata['status'] = re.search(r'\\*\\*Status:\\*\\*\\s*(.+)', line).group(1).strip()\n",
    "            \n",
    "            assets.append(metadata)\n",
    "    \n",
    "    # Extract audio assets\n",
    "    audio_section = manifest_data['sections'].get('audio_section')\n",
    "    if audio_section:\n",
    "        audio_matches = re.findall(asset_pattern, audio_section.group(0))\n",
    "        for match in audio_matches:\n",
    "            asset_id, filename, metadata_text = match\n",
    "            \n",
    "            # Parse metadata fields\n",
    "            metadata = {'category': 'audio', 'id': asset_id, 'filename': filename.strip()}\n",
    "            \n",
    "            for line in metadata_text.split('\\n'):\n",
    "                if '**Type:**' in line:\n",
    "                    metadata['type'] = re.search(r'\\*\\*Type:\\*\\*\\s*(.+)', line).group(1).strip()\n",
    "                elif '**Purpose:**' in line:\n",
    "                    metadata['purpose'] = re.search(r'\\*\\*Purpose:\\*\\*\\s*(.+)', line).group(1).strip()\n",
    "                elif '**Content:**' in line:\n",
    "                    metadata['content_description'] = re.search(r'\\*\\*Content:\\*\\*\\s*(.+)', line).group(1).strip()\n",
    "                elif '**Status:**' in line:\n",
    "                    metadata['status'] = re.search(r'\\*\\*Status:\\*\\*\\s*(.+)', line).group(1).strip()\n",
    "            \n",
    "            assets.append(metadata)\n",
    "    \n",
    "    return assets\n",
    "\n",
    "# Extract asset metadata\n",
    "extracted_assets = extract_asset_metadata(parsed_manifest)\n",
    "\n",
    "print(f\"ğŸ¯ Extracted {len(extracted_assets)} assets:\")\n",
    "for asset in extracted_assets:\n",
    "    print(f\"   {asset['category'].upper()}: {asset['filename'][:50]}...\")\n",
    "    print(f\"      Type: {asset.get('type', 'Unknown')}\")\n",
    "    print(f\"      Status: {asset.get('status', 'Unknown')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffe248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Asset Database Structure\n",
    "def create_asset_database(assets_data: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Build a structured database representation of TEC assets\"\"\"\n",
    "    \n",
    "    if not assets_data:\n",
    "        print(\"âš ï¸  No asset data available - creating empty database\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame with consistent columns\n",
    "    df = pd.DataFrame(assets_data)\n",
    "    \n",
    "    # Ensure all required columns exist\n",
    "    required_columns = ['category', 'id', 'filename', 'type', 'purpose', 'content_description', 'status']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 'Unknown'\n",
    "    \n",
    "    # Add computed fields\n",
    "    df['file_extension'] = df['filename'].apply(lambda x: Path(x).suffix.lower() if x else '')\n",
    "    df['estimated_path'] = df.apply(lambda row: f\"assets/{row['category']}/{row['filename']}\", axis=1)\n",
    "    df['asset_hash'] = df['filename'].apply(lambda x: hashlib.md5(str(x).encode()).hexdigest()[:8] if x else '')\n",
    "    df['processing_priority'] = df['type'].map({\n",
    "        'Core Mission Video': 1,\n",
    "        'Blueprint Audio': 1,\n",
    "        'Commercial/Narrative': 2,\n",
    "        'Director\\'s Commentary': 3,\n",
    "        'Extended Analysis': 3\n",
    "    }).fillna(4)\n",
    "    \n",
    "    # Clean and standardize data\n",
    "    df['status_clean'] = df['status'].str.replace('âœ… ', '').str.strip()\n",
    "    df['type_category'] = df['type'].apply(lambda x: 'Core' if 'Core' in str(x) or 'Blueprint' in str(x) else 'Supporting')\n",
    "    \n",
    "    return df.sort_values(['processing_priority', 'category', 'id'])\n",
    "\n",
    "def generate_database_summary(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Generate summary statistics for the asset database\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        return {'total_assets': 0, 'categories': {}, 'status_distribution': {}}\n",
    "    \n",
    "    summary = {\n",
    "        'total_assets': len(df),\n",
    "        'categories': df['category'].value_counts().to_dict(),\n",
    "        'status_distribution': df['status_clean'].value_counts().to_dict(),\n",
    "        'type_distribution': df['type'].value_counts().to_dict(),\n",
    "        'file_extensions': df['file_extension'].value_counts().to_dict(),\n",
    "        'priority_distribution': df['processing_priority'].value_counts().to_dict(),\n",
    "        'core_vs_supporting': df['type_category'].value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Create the asset database\n",
    "asset_db = create_asset_database(extracted_assets)\n",
    "db_summary = generate_database_summary(asset_db)\n",
    "\n",
    "print(\"ğŸ—„ï¸  TEC Asset Database Created:\")\n",
    "print(f\"   ğŸ“Š Total Assets: {db_summary['total_assets']}\")\n",
    "print(f\"   ğŸ“ Categories: {dict(db_summary['categories'])}\")\n",
    "print(f\"   âœ… Status: {dict(db_summary['status_distribution'])}\")\n",
    "print(f\"   ğŸ¯ Types: {dict(db_summary['type_distribution'])}\")\n",
    "\n",
    "if not asset_db.empty:\n",
    "    print(\"\\nğŸ“‹ Asset Database Preview:\")\n",
    "    display(asset_db[['category', 'filename', 'type', 'status_clean', 'processing_priority']].head())\n",
    "else:\n",
    "    print(\"âš ï¸  Database is empty - check manifest parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Asset Organization\n",
    "def validate_directory_structure() -> Dict:\n",
    "    \"\"\"Check if the expected directory structure exists\"\"\"\n",
    "    \n",
    "    base_path = Path(\"assets\")\n",
    "    expected_dirs = [\"video\", \"audio\"]\n",
    "    expected_files = [\"ASSET_MANIFEST.md\"]\n",
    "    \n",
    "    validation_results = {\n",
    "        'base_directory_exists': base_path.exists(),\n",
    "        'subdirectories': {},\n",
    "        'required_files': {},\n",
    "        'directory_structure_valid': True\n",
    "    }\n",
    "    \n",
    "    # Check subdirectories\n",
    "    for dir_name in expected_dirs:\n",
    "        dir_path = base_path / dir_name\n",
    "        validation_results['subdirectories'][dir_name] = {\n",
    "            'exists': dir_path.exists(),\n",
    "            'is_directory': dir_path.is_dir() if dir_path.exists() else False,\n",
    "            'file_count': len(list(dir_path.glob('*'))) if dir_path.exists() and dir_path.is_dir() else 0\n",
    "        }\n",
    "        \n",
    "        if not validation_results['subdirectories'][dir_name]['exists']:\n",
    "            validation_results['directory_structure_valid'] = False\n",
    "    \n",
    "    # Check required files\n",
    "    for file_name in expected_files:\n",
    "        file_path = base_path / file_name\n",
    "        validation_results['required_files'][file_name] = {\n",
    "            'exists': file_path.exists(),\n",
    "            'is_file': file_path.is_file() if file_path.exists() else False,\n",
    "            'size_bytes': file_path.stat().st_size if file_path.exists() and file_path.is_file() else 0\n",
    "        }\n",
    "        \n",
    "        if not validation_results['required_files'][file_name]['exists']:\n",
    "            validation_results['directory_structure_valid'] = False\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def validate_asset_files(asset_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Check if the actual asset files exist at their expected locations\"\"\"\n",
    "    \n",
    "    if asset_df.empty:\n",
    "        return {'validation_possible': False, 'message': 'No assets to validate'}\n",
    "    \n",
    "    file_validation = {\n",
    "        'validation_possible': True,\n",
    "        'total_expected': len(asset_df),\n",
    "        'files_found': 0,\n",
    "        'files_missing': 0,\n",
    "        'missing_files': [],\n",
    "        'found_files': [],\n",
    "        'validation_details': []\n",
    "    }\n",
    "    \n",
    "    for _, asset in asset_df.iterrows():\n",
    "        file_path = Path(asset['estimated_path'])\n",
    "        file_exists = file_path.exists()\n",
    "        \n",
    "        validation_detail = {\n",
    "            'filename': asset['filename'],\n",
    "            'expected_path': str(file_path),\n",
    "            'exists': file_exists,\n",
    "            'category': asset['category'],\n",
    "            'type': asset['type']\n",
    "        }\n",
    "        \n",
    "        if file_exists:\n",
    "            validation_detail['size_bytes'] = file_path.stat().st_size\n",
    "            file_validation['files_found'] += 1\n",
    "            file_validation['found_files'].append(asset['filename'])\n",
    "        else:\n",
    "            file_validation['files_missing'] += 1\n",
    "            file_validation['missing_files'].append(asset['filename'])\n",
    "        \n",
    "        file_validation['validation_details'].append(validation_detail)\n",
    "    \n",
    "    file_validation['completion_rate'] = (file_validation['files_found'] / file_validation['total_expected']) * 100\n",
    "    \n",
    "    return file_validation\n",
    "\n",
    "# Run validations\n",
    "directory_validation = validate_directory_structure()\n",
    "file_validation = validate_asset_files(asset_db)\n",
    "\n",
    "print(\"ğŸ” TEC Asset Organization Validation:\")\n",
    "print(f\"   ğŸ“ Directory Structure: {'âœ… Valid' if directory_validation['directory_structure_valid'] else 'âŒ Invalid'}\")\n",
    "print(f\"   ğŸ“‚ Base Directory: {'âœ… Exists' if directory_validation['base_directory_exists'] else 'âŒ Missing'}\")\n",
    "\n",
    "for dir_name, dir_info in directory_validation['subdirectories'].items():\n",
    "    status = 'âœ… OK' if dir_info['exists'] and dir_info['is_directory'] else 'âŒ Missing'\n",
    "    file_count = dir_info['file_count']\n",
    "    print(f\"   ğŸ“‚ {dir_name}/: {status} ({file_count} files)\")\n",
    "\n",
    "if file_validation['validation_possible']:\n",
    "    print(f\"\\nğŸ“„ File Validation Results:\")\n",
    "    print(f\"   ğŸ¯ Completion Rate: {file_validation['completion_rate']:.1f}%\")\n",
    "    print(f\"   âœ… Files Found: {file_validation['files_found']}\")\n",
    "    print(f\"   âŒ Files Missing: {file_validation['files_missing']}\")\n",
    "    \n",
    "    if file_validation['missing_files']:\n",
    "        print(f\"\\n   ğŸ“‹ Missing Files:\")\n",
    "        for missing_file in file_validation['missing_files'][:5]:  # Show first 5\n",
    "            print(f\"      - {missing_file}\")\n",
    "else:\n",
    "    print(\"âš ï¸  File validation skipped - no asset database available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Asset Reports\n",
    "def create_asset_summary_report(asset_df: pd.DataFrame, validation_data: Dict) -> str:\n",
    "    \"\"\"Generate a comprehensive text-based asset report\"\"\"\n",
    "    \n",
    "    report_lines = [\n",
    "        \"=\" * 80,\n",
    "        \"ğŸ›ï¸  TEC ASSET MANAGEMENT SUMMARY REPORT\",\n",
    "        \"=\" * 80,\n",
    "        f\"ğŸ“… Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        f\"ğŸ¯ Analysis Phase: Implementation Ready\",\n",
    "        \"\",\n",
    "        \"ğŸ“Š ASSET OVERVIEW:\",\n",
    "        f\"   Total Assets: {len(asset_df) if not asset_df.empty else 0}\",\n",
    "    ]\n",
    "    \n",
    "    if not asset_df.empty:\n",
    "        # Asset breakdown\n",
    "        for category, count in asset_df['category'].value_counts().items():\n",
    "            report_lines.append(f\"   {category.title()} Assets: {count}\")\n",
    "        \n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"ğŸ­ ASSET TYPES:\",\n",
    "        ])\n",
    "        \n",
    "        for asset_type, count in asset_df['type'].value_counts().items():\n",
    "            report_lines.append(f\"   â€¢ {asset_type}: {count}\")\n",
    "        \n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"âš¡ PROCESSING PRIORITY DISTRIBUTION:\",\n",
    "        ])\n",
    "        \n",
    "        priority_map = {1: \"Critical\", 2: \"High\", 3: \"Medium\", 4: \"Low\"}\n",
    "        for priority, count in sorted(asset_df['processing_priority'].value_counts().items()):\n",
    "            priority_name = priority_map.get(priority, f\"Priority {priority}\")\n",
    "            report_lines.append(f\"   â€¢ {priority_name}: {count} assets\")\n",
    "    \n",
    "    # Validation summary\n",
    "    if validation_data.get('validation_possible', False):\n",
    "        completion_rate = validation_data.get('completion_rate', 0)\n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"ğŸ” VALIDATION STATUS:\",\n",
    "            f\"   File Completion: {completion_rate:.1f}%\",\n",
    "            f\"   Files Found: {validation_data.get('files_found', 0)}\",\n",
    "            f\"   Files Missing: {validation_data.get('files_missing', 0)}\",\n",
    "        ])\n",
    "        \n",
    "        if validation_data.get('missing_files'):\n",
    "            report_lines.extend([\n",
    "                \"\",\n",
    "                \"âŒ MISSING ASSETS:\",\n",
    "            ])\n",
    "            for missing_file in validation_data['missing_files'][:10]:\n",
    "                report_lines.append(f\"   â€¢ {missing_file}\")\n",
    "    \n",
    "    report_lines.extend([\n",
    "        \"\",\n",
    "        \"ğŸš€ READINESS ASSESSMENT:\",\n",
    "        f\"   Directory Structure: {'âœ… Valid' if validation_data.get('directory_structure_valid', False) else 'âŒ Needs Setup'}\",\n",
    "        f\"   Asset Organization: {'âœ… Complete' if validation_data.get('completion_rate', 0) == 100 else 'âš ï¸  In Progress'}\",\n",
    "        f\"   Integration Ready: {'âœ… Yes' if validation_data.get('completion_rate', 0) >= 80 else 'âŒ Pending'}\",\n",
    "        \"\",\n",
    "        \"=\" * 80\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "def export_asset_database(asset_df: pd.DataFrame, output_format: str = 'json') -> str:\n",
    "    \"\"\"Export the asset database in various formats\"\"\"\n",
    "    \n",
    "    if asset_df.empty:\n",
    "        return \"No asset data to export\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    if output_format.lower() == 'json':\n",
    "        output_file = f\"tec_assets_{timestamp}.json\"\n",
    "        asset_dict = asset_df.to_dict('records')\n",
    "        \n",
    "        export_data = {\n",
    "            'metadata': {\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'total_assets': len(asset_df),\n",
    "                'export_format': 'json',\n",
    "                'source': 'TEC Asset Management System'\n",
    "            },\n",
    "            'assets': asset_dict\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return f\"âœ… Exported to {output_file}\"\n",
    "    \n",
    "    elif output_format.lower() == 'csv':\n",
    "        output_file = f\"tec_assets_{timestamp}.csv\"\n",
    "        asset_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        return f\"âœ… Exported to {output_file}\"\n",
    "    \n",
    "    else:\n",
    "        return f\"âŒ Unsupported format: {output_format}\"\n",
    "\n",
    "# Generate reports\n",
    "summary_report = create_asset_summary_report(asset_db, file_validation)\n",
    "print(summary_report)\n",
    "\n",
    "# Export options\n",
    "if not asset_db.empty:\n",
    "    print(\"\\nğŸ“¤ Export Options Available:\")\n",
    "    print(\"   â€¢ JSON format for system integration\")\n",
    "    print(\"   â€¢ CSV format for spreadsheet analysis\")\n",
    "    print(\"   â€¢ Run export_asset_database(asset_db, 'json') to export\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No data available for export - check asset parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3057312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Asset Integration Framework\n",
    "def create_axiom_engine_integration(asset_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Prepare asset data structures for AxiomEngine validation framework\"\"\"\n",
    "    \n",
    "    if asset_df.empty:\n",
    "        return {'integration_ready': False, 'message': 'No assets available for integration'}\n",
    "    \n",
    "    # Map assets to AxiomEngine content types\n",
    "    content_type_mapping = {\n",
    "        'Core Mission Video': 'narrative',\n",
    "        'Blueprint Audio': 'policy', \n",
    "        'Commercial/Narrative': 'story',\n",
    "        'Director\\'s Commentary': 'dialogue',\n",
    "        'Extended Analysis': 'decision'\n",
    "    }\n",
    "    \n",
    "    integration_data = {\n",
    "        'integration_ready': True,\n",
    "        'total_assets': len(asset_df),\n",
    "        'axiom_content_types': {},\n",
    "        'processing_queue': [],\n",
    "        'validation_priorities': {},\n",
    "        'asset_metadata_for_engine': []\n",
    "    }\n",
    "    \n",
    "    # Process each asset for AxiomEngine integration\n",
    "    for _, asset in asset_df.iterrows():\n",
    "        axiom_content_type = content_type_mapping.get(asset['type'], 'narrative')\n",
    "        \n",
    "        # Build metadata structure for AxiomEngine\n",
    "        engine_metadata = {\n",
    "            'asset_id': asset['asset_hash'],\n",
    "            'filename': asset['filename'],\n",
    "            'content_type': axiom_content_type,\n",
    "            'validation_priority': asset['processing_priority'],\n",
    "            'category': asset['category'],\n",
    "            'purpose': asset.get('purpose', ''),\n",
    "            'estimated_path': asset['estimated_path'],\n",
    "            'integration_status': 'ready' if asset['status_clean'] == 'Organized & Ready' else 'pending'\n",
    "        }\n",
    "        \n",
    "        integration_data['asset_metadata_for_engine'].append(engine_metadata)\n",
    "        \n",
    "        # Update content type distribution\n",
    "        if axiom_content_type not in integration_data['axiom_content_types']:\n",
    "            integration_data['axiom_content_types'][axiom_content_type] = 0\n",
    "        integration_data['axiom_content_types'][axiom_content_type] += 1\n",
    "        \n",
    "        # Add to processing queue if ready\n",
    "        if engine_metadata['integration_status'] == 'ready':\n",
    "            integration_data['processing_queue'].append({\n",
    "                'asset_id': asset['asset_hash'],\n",
    "                'filename': asset['filename'],\n",
    "                'content_type': axiom_content_type,\n",
    "                'priority': asset['processing_priority']\n",
    "            })\n",
    "    \n",
    "    # Sort processing queue by priority\n",
    "    integration_data['processing_queue'].sort(key=lambda x: x['priority'])\n",
    "    \n",
    "    return integration_data\n",
    "\n",
    "def generate_integration_workflow(integration_data: Dict) -> List[str]:\n",
    "    \"\"\"Generate step-by-step integration workflow for TEC assets\"\"\"\n",
    "    \n",
    "    if not integration_data.get('integration_ready', False):\n",
    "        return [\"âŒ Integration not possible - no asset data available\"]\n",
    "    \n",
    "    workflow_steps = [\n",
    "        \"ğŸ›ï¸  TEC ASSET INTEGRATION WORKFLOW\",\n",
    "        \"=\" * 50,\n",
    "        \"\",\n",
    "        \"PHASE 1: Asset Preparation\",\n",
    "        \"1. âœ… Asset manifest parsed and validated\",\n",
    "        \"2. âœ… Database structure created\",\n",
    "        \"3. âœ… Content types mapped to AxiomEngine\",\n",
    "        \"\",\n",
    "        \"PHASE 2: AxiomEngine Integration\",\n",
    "    ]\n",
    "    \n",
    "    # Add steps for each content type\n",
    "    content_types = integration_data.get('axiom_content_types', {})\n",
    "    for content_type, count in content_types.items():\n",
    "        workflow_steps.append(f\"4. Process {count} {content_type} assets through validation\")\n",
    "    \n",
    "    workflow_steps.extend([\n",
    "        \"\",\n",
    "        \"PHASE 3: Validation Framework\",\n",
    "        \"5. Initialize AxiomEngine with 8 Foundational Principles\",\n",
    "        \"6. Configure content-specific validation rules\",\n",
    "        \"7. Set up real-time validation pipeline\",\n",
    "        \"\",\n",
    "        \"PHASE 4: System Integration\",\n",
    "        \"8. Connect assets to TECSystem orchestrator\",\n",
    "        \"9. Enable DialogueInterface for human-AI collaboration\", \n",
    "        \"10. Deploy to Azure Container Apps infrastructure\",\n",
    "        \"\",\n",
    "        f\"ğŸ“Š READY FOR PROCESSING: {len(integration_data.get('processing_queue', []))} assets in queue\",\n",
    "        \"ğŸš€ NEXT: Run AxiomEngine validation on priority assets\"\n",
    "    ])\n",
    "    \n",
    "    return workflow_steps\n",
    "\n",
    "def create_asset_api_endpoints(integration_data: Dict) -> Dict:\n",
    "    \"\"\"Design API endpoints for asset management within TEC system\"\"\"\n",
    "    \n",
    "    api_design = {\n",
    "        'base_url': '/api/assets',\n",
    "        'endpoints': {\n",
    "            '/api/assets/list': {\n",
    "                'method': 'GET',\n",
    "                'description': 'List all managed assets with metadata',\n",
    "                'response_type': 'AssetListResponse'\n",
    "            },\n",
    "            '/api/assets/{asset_id}': {\n",
    "                'method': 'GET', \n",
    "                'description': 'Get detailed information about specific asset',\n",
    "                'response_type': 'AssetDetailResponse'\n",
    "            },\n",
    "            '/api/assets/{asset_id}/validate': {\n",
    "                'method': 'POST',\n",
    "                'description': 'Run AxiomEngine validation on asset content',\n",
    "                'response_type': 'ValidationResult'\n",
    "            },\n",
    "            '/api/assets/search': {\n",
    "                'method': 'POST',\n",
    "                'description': 'Search assets by content type, purpose, or metadata',\n",
    "                'response_type': 'SearchResponse'\n",
    "            },\n",
    "            '/api/assets/integration/status': {\n",
    "                'method': 'GET',\n",
    "                'description': 'Get overall integration status and readiness',\n",
    "                'response_type': 'IntegrationStatusResponse'\n",
    "            }\n",
    "        },\n",
    "        'data_structures': {\n",
    "            'AssetMetadata': {\n",
    "                'asset_id': 'string',\n",
    "                'filename': 'string', \n",
    "                'content_type': 'AxiomContentType',\n",
    "                'validation_priority': 'number',\n",
    "                'integration_status': 'string'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return api_design\n",
    "\n",
    "# Create integration framework\n",
    "integration_framework = create_axiom_engine_integration(asset_db)\n",
    "workflow_steps = generate_integration_workflow(integration_framework)\n",
    "api_design = create_asset_api_endpoints(integration_framework)\n",
    "\n",
    "print(\"ğŸ”§ TEC Asset Integration Framework:\")\n",
    "print(f\"   Integration Ready: {'âœ… Yes' if integration_framework.get('integration_ready') else 'âŒ No'}\")\n",
    "\n",
    "if integration_framework.get('integration_ready'):\n",
    "    print(f\"   Assets for Processing: {integration_framework['total_assets']}\")\n",
    "    print(f\"   Content Types: {dict(integration_framework['axiom_content_types'])}\")\n",
    "    print(f\"   Processing Queue: {len(integration_framework['processing_queue'])} items\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ Integration Workflow:\")\n",
    "    for step in workflow_steps[:15]:  # Show first 15 steps\n",
    "        print(f\"   {step}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”Œ API Endpoints Designed: {len(api_design['endpoints'])} endpoints\")\n",
    "    print(\"   Ready for integration with TEC server architecture\")\n",
    "else:\n",
    "    print(\"   âš ï¸  Integration framework requires asset data to proceed\")\n",
    "\n",
    "print(\"\\nğŸ¯ INTEGRATION STATUS: Framework ready for AxiomEngine validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c3f12",
   "metadata": {},
   "source": [
    "# ğŸ¯ TEC Asset Management - Complete\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook has successfully transformed the chaotic creative energy of your foundational media assets into a systematic digital asset management framework. \n",
    "\n",
    "### âœ… Accomplished:\n",
    "\n",
    "1. **ğŸ“‹ Asset Manifest Analysis** - Parsed your ASSET_MANIFEST.md and extracted structured metadata\n",
    "2. **ğŸ—„ï¸ Database Creation** - Built pandas-based asset database with categorization and indexing  \n",
    "3. **ğŸ” Validation Framework** - Implemented directory structure and file existence validation\n",
    "4. **ğŸ“Š Reporting System** - Generated comprehensive asset reports and export capabilities\n",
    "5. **ğŸ”§ Integration Framework** - Prepared data structures for AxiomEngine validation workflow\n",
    "\n",
    "### ğŸš€ Ready For:\n",
    "\n",
    "- **AxiomEngine Integration** - Assets mapped to content types (narrative, policy, story, dialogue, decision)\n",
    "- **TEC Server Integration** - API endpoints designed for asset management within the TEC system\n",
    "- **Azure Deployment** - Framework ready for cloud infrastructure deployment\n",
    "- **Real-time Validation** - Processing queue prepared for live constitutional framework validation\n",
    "\n",
    "### ğŸ“ Your Assets:\n",
    "\n",
    "**Video Assets:**\n",
    "- The Elidoras Codex Blueprint (Core Mission)\n",
    "- Mind F*cked by the Universe Enterprises (Commercial/Narrative)\n",
    "\n",
    "**Audio Assets:**  \n",
    "- Architecting a New Civilization (Blueprint Audio)\n",
    "- Corporate Hypocrisy Commentary (Director's Commentary)\n",
    "- Civilizational Lifeboat Analysis (Extended Analysis)\n",
    "\n",
    "---\n",
    "\n",
    "**The machine that processes chaos into constitutional order is ready.** ğŸ›ï¸âš¡\n",
    "\n",
    "Your foundational media assets are now systematically organized and prepared for integration with the TEC AxiomEngine validation framework."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
